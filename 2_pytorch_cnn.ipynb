{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A convolutional neural network written in pure tensorflow.\n",
    "\n",
    "**Note:** this solution tries to follow the tensorflow CNN solution as close as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "39fba1ef-ec4b-38ca-fca0-0bdb31e294f4"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6efa16c1-f457-45bb-49a3-69e11c7fae18"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "from IPython.display import clear_output\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "from fetch_mnist import fetch_mnist\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data parameters\n",
    "num_labels = 10 # Number of different types of labels (1-10)\n",
    "width, height = 28, 28 # width / height of the image\n",
    "depth = 1 # Number of colors in the image (greyscale)\n",
    "\n",
    "# Training Parameters\n",
    "num_steps = 20000   # Number of training steps to run\n",
    "test_size = 10000 # Test data set size\n",
    "valid_size = 10000 # Validation data set size\n",
    "batch_size = 100 # Stochastic Gradient Descent batch size\n",
    "\n",
    "# CNN Parameters\n",
    "kernel_size = 5 # Convolutional Kernel size\n",
    "kernel_depth = 32 # Convolutional Kernel depth size == Number of Convolutional Kernels\n",
    "num_hidden = 1024 # Number of hidden neurons in the fully connected layer\n",
    "\n",
    "# Optimization parameters\n",
    "learning_rate = 0.0001 # Learning rate\n",
    "\n",
    "# Cuda\n",
    "cuda = True # use GPU or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable(x):\n",
    "    ''' Convert an array to a torch variable'''\n",
    "    if not torch.is_tensor(x):\n",
    "        x = torch.from_numpy(np.asarray(x))\n",
    "    var = Variable(x)\n",
    "    if cuda:\n",
    "        var = var.cuda()\n",
    "    return var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f731214e-9fca-f54d-91be-60475207ba64"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the custom MNIST data fetcher from `fetch_mnist.py`.\n",
    "\n",
    "As a preprocessing step, the labels are converted to a one-hot encoded sparse matrix. [One hot encoding](https://en.wikipedia.org/wiki/One-hot) is just a way of replacing each integer in the label vector with a row of zeros, except at the position specified by the integer, where you put a 1.\n",
    "\n",
    "Note that the inverse transformation of the one-hot encoding can always be performed by taking the `argmax` along `axis=1`.\n",
    "\n",
    "Secondly, the image values are specified by an integer between 0 and 255. We convert these pixel values to a float between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "cc368a9d-79d3-6e70-1732-4ebdd696a409"
   },
   "outputs": [],
   "source": [
    "train_data, test_data = fetch_mnist()\n",
    "data = np.vstack([train_data, test_data])\n",
    "np.random.shuffle(data)\n",
    "\n",
    "train_data = data[:-test_size-valid_size]\n",
    "valid_data = data[-test_size-valid_size:-test_size]\n",
    "test_data  = data[-test_size:]\n",
    "\n",
    "def get_values_labels(data):\n",
    "    labels = np.int64(data[:,0])\n",
    "    labels_onehot = np.zeros((len(labels), num_labels))\n",
    "    labels_onehot[np.arange(len(labels)),labels] = 1\n",
    "    values = np.float32(data[:,1:]).reshape(-1, depth, height, width)/255\n",
    "    return variable(values), variable(labels), variable(labels_onehot)\n",
    "    \n",
    "train_values, train_labels, train_labels_onehot = get_values_labels(train_data)\n",
    "valid_values, valid_labels, valid_labels_onehot = get_values_labels(valid_data)\n",
    "test_values, test_labels, test_labels_onehot = get_values_labels(test_data)\n",
    "\n",
    "print(f'train data shape:\\t\\t{train_values.shape}')\n",
    "print(f'train labels (one-hot) shape:\\t{train_labels_onehot.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the different digits by writing a visualization function that reshapes the 784D train and test values into a 28x28 grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_digit(digit_array):\n",
    "    plt.imshow(digit_array.view(height, width).data.cpu().numpy(), cmap='Greys')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()\n",
    "    \n",
    "show_digit(train_values[31])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now build a network with two convolutional layers, followed by two fully connected layers. We use the `torch.nn.Module` to create the network.\n",
    "\n",
    "We choose a 4 layered network consisting of 2 convolutional layers with kernel size `kernel_size` and depth `kernel_depth` and `2*kernel_depth` respectively. These convolutional layers are followed by two fully connected layers with `num_hidden` hidden neurons `num_labels` output nodes (one-hot encoding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        # First Convoluational Layer\n",
    "        self.conv1 = torch.nn.Conv2d(\n",
    "            in_channels=depth,\n",
    "            out_channels=kernel_depth,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=1,\n",
    "            padding=(kernel_size-1)//2, # = 'SAME'\n",
    "        )\n",
    "        self.mp1 = torch.nn.MaxPool2d(\n",
    "            kernel_size=2,\n",
    "        )\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        # Second Convolutional Layer\n",
    "        self.conv2 = torch.nn.Conv2d(\n",
    "            in_channels=kernel_depth,\n",
    "            out_channels=2*kernel_depth,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=1,\n",
    "            padding=(kernel_size-1)//2, # = 'SAME'\n",
    "        )\n",
    "        self.mp2 = torch.nn.MaxPool2d(\n",
    "            kernel_size=2,\n",
    "        )\n",
    "        self.relu2 = torch.nn.ReLU()\n",
    "        # First Fully Connected Layer\n",
    "        self.fc1 = torch.nn.Linear(\n",
    "            in_features=width//4*width//4*2*kernel_depth,\n",
    "            out_features=num_hidden,\n",
    "        )\n",
    "        self.relu3 = torch.nn.ReLU()\n",
    "        # Second Fully Connected Layer\n",
    "        self.fc2 = torch.nn.Linear(\n",
    "            in_features=num_hidden,\n",
    "            out_features=num_labels\n",
    "        )\n",
    "        self.softmax = torch.nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        # first convolutional layer:\n",
    "        x = self.relu1(self.mp1(self.conv1(data)))\n",
    "        # second convolutional layer:\n",
    "        x = self.relu2(self.mp2(self.conv2(x)))\n",
    "        # first fully connected layer:\n",
    "        x = self.relu3(self.fc1(x.view(x.size(0),-1)))\n",
    "        # second fully connected layer:\n",
    "        logits = self.fc2(x)\n",
    "        prediction = self.softmax(logits)\n",
    "        return logits, prediction\n",
    "    \n",
    "model = Model()\n",
    "if cuda:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the categorical cross entropy loss for training the model.\n",
    "\n",
    "As optimizer we could use a Gradient Descent optimizer [with or without decaying learning rate] or one of the more sophisticated (and easier to optimize) optimizers like Adam or RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "lossfunc = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# accuracy\n",
    "def accuracy(prediction, labels):\n",
    "    return 100*np.mean(np.argmax(prediction.data.cpu().numpy(), 1) == labels.data.cpu().numpy())\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e33cb0c7-51ed-02ae-ebb2-dcda12832da6"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4dbef346-feb4-1479-62a8-042785666e66"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "ss = ShuffleSplit(n_splits=num_steps, train_size=batch_size)\n",
    "history = [(0, np.nan, 10)] # Initial Error Measures\n",
    "for step, (idx, _) in enumerate(ss.split(train_data,train_labels), start=1):\n",
    "    optimizer.zero_grad()\n",
    "    idx = variable(idx)\n",
    "    batch_values = train_values[idx]\n",
    "    batch_labels = train_labels[idx]\n",
    "    logits, prediction = model(batch_values)\n",
    "    loss = lossfunc(logits, batch_labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step%500 == 0:\n",
    "        valid_logits, valid_prediction = model(valid_values)\n",
    "        # get element of 0D array:\n",
    "        valid_loss = lossfunc(valid_logits, valid_labels).item()\n",
    "        valid_accuracy = accuracy(valid_prediction, valid_labels)\n",
    "        history.append((step, valid_loss, valid_accuracy))\n",
    "        print(f'Step {step}\\t Valid. Acc. = {valid_accuracy}')\n",
    "        # clear GPU memory\n",
    "        del valid_logits, valid_prediction, valid_loss, valid_accuracy\n",
    "        \n",
    "    # clear GPU memory\n",
    "    del loss, logits, prediction, idx, batch_labels, batch_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps, loss, acc = zip(*history)\n",
    "\n",
    "fig, ax_loss = plt.subplots()\n",
    "ax_acc = ax_loss.twinx()\n",
    "\n",
    "plt.sca(ax_acc)\n",
    "plt.plot(steps, acc, '-o', color='C1')\n",
    "plt.ylabel('Accuracy [%]', color='C1');\n",
    "plt.tick_params('y', colors='C1')\n",
    "plt.ylim(98,100)\n",
    "plt.yticks([98,99,100])\n",
    "\n",
    "plt.sca(ax_loss)\n",
    "plt.plot(steps, loss, '-o', color='C0')\n",
    "plt.ylabel('Log Loss', color='C0');\n",
    "plt.tick_params('y', colors='C0')\n",
    "plt.ylim(0.01, 0.3)\n",
    "\n",
    "plt.xlim(0, max(steps))\n",
    "plt.xticks([0,num_steps//4, num_steps//2, 3*num_steps//4, num_steps])\n",
    "plt.xlabel('Training Steps')\n",
    "plt.title('Validation Loss / Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the accuracy on the test set can be evaluated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, test_prediction = model(test_values)\n",
    "test_accuracy = accuracy(test_prediction, test_labels)\n",
    "print(f'Test Accuracy = {test_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 315\n",
    "show_digit(test_values[N])\n",
    "_, prediction_onehot = model(test_values[N:N+1])\n",
    "prediction = np.argmax(prediction_onehot.data.cpu().numpy().ravel())\n",
    "target = test_labels[N].item()\n",
    "print(f'prediction={prediction}\\ttarget={target}')"
   ]
  }
 ],
 "metadata": {
  "_change_revision": 0,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
