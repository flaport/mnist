{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A convolutional neural network written in pure tensorflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "39fba1ef-ec4b-38ca-fca0-0bdb31e294f4"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "6efa16c1-f457-45bb-49a3-69e11c7fae18"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from fetch_mnist import fetch_mnist\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data parameters\n",
    "num_labels = 10 # Number of different types of labels (1-10)\n",
    "num_pixels = 28*28\n",
    "\n",
    "# Training Parameters\n",
    "num_steps = 10000   # Number of training steps to run\n",
    "test_size = 10000 # Test data set size\n",
    "valid_size = 5000 # Validation data set size\n",
    "train_size = 60000 - valid_size # Size of the training set\n",
    "batch_size = 128 # Stochastic Gradient Descent batch size\n",
    "\n",
    "# CNN Parameters\n",
    "num_inputs = 1\n",
    "num_timesteps = num_pixels\n",
    "num_hidden_rnn = 80\n",
    "num_layers_rnn = 1\n",
    "\n",
    "# Optimization parameters\n",
    "learning_rate = 0.0001 # Learning rate\n",
    "decay_rate = 0.9 # learning rate decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f731214e-9fca-f54d-91be-60475207ba64"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the custom MNIST data fetcher from `fetch_mnist.py`.\n",
    "\n",
    "As a preprocessing step, the labels are converted to a one-hot encoded sparse matrix. [One hot encoding](https://en.wikipedia.org/wiki/One-hot) is just a way of replacing each integer in the label vector with a row of zeros, except at the position specified by the integer, where you put a 1.\n",
    "\n",
    "Note that the inverse transformation of the one-hot encoding can always be performed by taking the `argmax` along `axis=1`.\n",
    "\n",
    "Secondly, the image values are specified by an integer between 0 and 255. We convert these pixel values to a float between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast with the Convolutional Neural Networks, we do a pixel-by-pixel recognition of the digit image. This is a good benchmark task for a recurrent neural network. The performance of this architecture will obviously be worse than for a convnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "cc368a9d-79d3-6e70-1732-4ebdd696a409"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape:\t\t(55000, 784, 1)\n",
      "train labels (one-hot) shape:\t(55000, 10)\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = fetch_mnist()\n",
    "data = np.vstack([train_data, test_data])\n",
    "np.random.shuffle(data)\n",
    "\n",
    "train_data = data[:-test_size-valid_size]\n",
    "valid_data = data[-test_size-valid_size:-test_size]\n",
    "test_data  = data[-test_size:]\n",
    "\n",
    "def get_values_labels(data):\n",
    "    labels = np.int32(data[:,0])\n",
    "    labels_onehot = np.zeros((len(labels), num_labels))\n",
    "    labels_onehot[np.arange(len(labels)),labels] = 1\n",
    "    values = np.float32(data[:,1:]).reshape(-1, num_timesteps, num_inputs)/255\n",
    "    return values, labels, labels_onehot\n",
    "\n",
    "train_values, train_labels, train_labels_onehot = get_values_labels(train_data)\n",
    "valid_values, valid_labels, valid_labels_onehot = get_values_labels(valid_data)\n",
    "test_values, test_labels, test_labels_onehot = get_values_labels(test_data)\n",
    "\n",
    "print(f'train data shape:\\t\\t{train_values.shape}')\n",
    "print(f'train labels (one-hot) shape:\\t{train_labels_onehot.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the different digits by writing a visualization function that reshapes the 784D train and test values into a 28x28 grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO4AAADuCAYAAAA+7jsiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAABrVJREFUeJzt3U+IjX8fxvF7Hr9osqAGicUMWVkoNeVPVqRmZ2Ez29mRpJSmxoZSFuRPWYgmNixsbCQLG5JJjdmQjYiyUM8oJWlC51k/i/M5M3Mc5vp5vbbX3Pc5pt7d6tuZ09dqtRogy3/+9BsAFk64EEi4EEi4EEi4EEi4EEi4EEi4EEi4EOifhfzwmjVrWkNDQz16K8C7d++a2dnZvk4/t6Bwh4aGmunp6cW/K6A0PDw8r5/zX2UIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwI9M+ffgN05927d+V+9+7dcr9x40a5v3z5cqFv6Zc5ceJE2+3YsWPltRs2bPjVb2dJ8cSFQMKFQMKFQMKFQMKFQMKFQI6DfoOfP3+We3XkMjY2tuhr5/Pa69atK/f169eXe2VgYKDcd+7cWe7nzp1ru92/f7+89sWLF+WezhMXAgkXAgkXAgkXAgkXAgkXAgkXAjnH/Q0mJibK/fz584u+96pVq8r99u3b5T4yMrLo1+61ycnJttubN2/Ka9+/f1/ug4ODi3pPS4UnLgQSLgQSLgQSLgQSLgQSLgQSLgRyjjsPP378KPdTp06Ve/W50qZpmr6+vrZbp3PaJ0+elPvWrVvLPdXc3Fy5z87OlrtzXOC3Ey4EEi4EEi4EEi4EEi4EEi4Eco47D3fu3Cn3s2fPdnX/ffv2td1u3bpVXrt27dquXruXXr16Ve7j4+OLvnd/f3+5d/P3oBN44kIg4UIg4UIg4UIg4UIg4UIg4UIg57hN03z79q3cjx49Wu6tVqvcO32m9tKlS223Xp/Tfvr0qdyfPn3adut0xtzp/LuT6vd6/fr18tqNGzd29dpLnScuBBIuBBIuBBIuBBIuBBIuBHIc1DTNyZMny/3z58/lXv151aZpmnXr1pX79PT0oramaZqpqalyf/bsWbl3+jrKTv/2SqffSyfPnz9vu23btq2re6fzxIVAwoVAwoVAwoVAwoVAwoVAwoVAznGbptmzZ0+5X758uav7v379utzHxsa6un83On0ksduz2MqWLVvKffv27T177XSeuBBIuBBIuBBIuBBIuBBIuBBIuBDIOW7TNAcOHCj3Y8eOlfvjx4/LfWZmptyrz+t2+2dGO31u9ebNm13dv7JixYpyf/DgQc9e+9/OExcCCRcCCRcCCRcCCRcCCRcCCRcCOcdtmmbZsmXlfuHCha7u//bt23IfGBhou3X6is5OZmdny72X57ijo6PlvmnTpp699r+dJy4EEi4EEi4EEi4EEi4EEi4EEi4Eco77G2zevLln93758mW5j4yMlHunv6tcmZiYKPczZ84s+t7UPHEhkHAhkHAhkHAhkHAhkHAhkOOgJe7jx4/lvnv37nL/+vVruXf6Gs3Tp0+33cbHx8tr6R1PXAgkXAgkXAgkXAgkXAgkXAgkXAjkHHcJmJuba7vt2LGjvLbTOW23Dh061HZbvnx5T1+b9jxxIZBwIZBwIZBwIZBwIZBwIZBwIZBz3N/g+/fv5X78+PG224cPH3712/k/Dx8+LPfqK0D5czxxIZBwIZBwIZBwIZBwIZBwIZBwIZBz3N/g0aNH5X716tWevfbZs2fLfe/evT17bXrHExcCCRcCCRcCCRcCCRcCCRcCOQ76BV6/fl3u+/fvL/dOX3VZGR0dLfcjR44s+t4sXZ64EEi4EEi4EEi4EEi4EEi4EEi4EMg57jx8+fKl3MfGxsq90zltta9YsaK8dnJystz7+/vLnUyeuBBIuBBIuBBIuBBIuBBIuBBIuBDIOe48XLt2rdynpqa6uv/q1avbbjMzM+W1zmn/Tp64EEi4EEi4EEi4EEi4EEi4EEi4EMg57jxcuXKlq+tXrlxZ7vfu3Wu7DQ4OdvXa/Dt54kIg4UIg4UIg4UIg4UIg4UIg4UIg57jzcPDgwXK/ePFiuR8+fLjcd+3ateD3xN/NExcCCRcCCRcCCRcCCRcCCRcC9bVarXn/8PDwcGt6erqHbwf+bsPDw8309HT9vayNJy5EEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4EWtDncfv6+v7bNM373r0d+OsNtlqttZ1+aEHhAkuD/ypDIOFCIOFCIOFCIOFCIOFCIOFCIOFCIOFCoP8Bi+YAHA8IuFMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_digit(digit_array):\n",
    "    plt.imshow(digit_array.reshape(28, 28), cmap='Greys')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()\n",
    "    \n",
    "show_digit(train_values[31])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self):\n",
    "        # inputs\n",
    "        self.data = tf.placeholder(tf.float32, [None, num_timesteps, num_inputs])\n",
    "        self.labels = tf.placeholder(tf.int64, [None])\n",
    "        \n",
    "        # weights and biases\n",
    "        w_init_val = np.sqrt(6.)/np.sqrt(num_labels + num_inputs)\n",
    "        self.w_rnn_out = tf.get_variable(\"w_rnn_out\", shape = [num_hidden_rnn, num_labels], dtype=tf.float32, initializer=tf.random_uniform_initializer(-w_init_val, w_init_val))\n",
    "        self.b_rnn_out = tf.get_variable(\"b_rnn_out\", shape=[num_labels], dtype=tf.float32, initializer=tf.constant_initializer(0.01))\n",
    "        \n",
    "        # recurrent layer\n",
    "        rnn_layer = lambda : tf.contrib.rnn.BasicLSTMCell(num_hidden_rnn)\n",
    "        self.rnn_cell = tf.contrib.rnn.MultiRNNCell([rnn_layer() for _ in range(num_layers_rnn)])\n",
    "        rnn_out, _ = tf.nn.dynamic_rnn(self.rnn_cell, self.data, dtype=tf.float32)\n",
    "\n",
    "        # output layer\n",
    "        logits = tf.matmul(rnn_out[:,-1,:], self.w_rnn_out) + self.b_rnn_out\n",
    "        \n",
    "        # prediction\n",
    "        self.pred = tf.nn.softmax(logits)\n",
    "\n",
    "        # loss & accuracy\n",
    "        self.loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=self.labels))\n",
    "        self.acc = 100*tf.reduce_mean(tf.cast(tf.equal(tf.argmax(logits, 1), self.labels), tf.float32))\n",
    "        \n",
    "        # optimizer\n",
    "        self.opt = tf.train.RMSPropOptimizer(learning_rate=learning_rate, decay=decay_rate).minimize(self.loss)\n",
    "        \n",
    "        # initializer\n",
    "        self.init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e33cb0c7-51ed-02ae-ebb2-dcda12832da6"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.Session()\n",
    "session.run(model.init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step     0\t Valid. Acc. =  8.74\n",
      "Step   429\t Valid. Acc. = 19.44\n",
      "Step   858\t Valid. Acc. = 22.68\n",
      "Step  1287\t Valid. Acc. = 30.56\n",
      "Step  1716\t Valid. Acc. = 32.08\n",
      "Step  2145\t Valid. Acc. = 29.08\n",
      "Step  2574\t Valid. Acc. = 38.74\n",
      "Step  3003\t Valid. Acc. = 36.74\n",
      "Step  3432\t Valid. Acc. = 40.34\n",
      "Step  3861\t Valid. Acc. = 45.86\n",
      "Step  4290\t Valid. Acc. = 45.54\n",
      "Step  4719\t Valid. Acc. = 49.48\n",
      "Step  5148\t Valid. Acc. = 48.68\n",
      "Step  5577\t Valid. Acc. = 49.04\n",
      "Step  6006\t Valid. Acc. = 49.18\n",
      "Step  6435\t Valid. Acc. = 48.16\n",
      "Step  6864\t Valid. Acc. = 45.78\n",
      "Step  7293\t Valid. Acc. = 44.24\n",
      "Step  7722\t Valid. Acc. = 47.40\n",
      "Step  8151\t Valid. Acc. = 47.70\n",
      "Step  8580\t Valid. Acc. = 47.68\n",
      "Step  9009\t Valid. Acc. = 49.04\n",
      "Step  9438\t Valid. Acc. = 48.92\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = []\n",
    "for step in range(num_steps):\n",
    "    idxs = np.random.randint(0, train_size, batch_size)\n",
    "\n",
    "    if step%(train_size//batch_size) == 0 or step == num_steps -1:\n",
    "        val_acc = np.zeros(valid_size//100)\n",
    "        val_loss = np.zeros(valid_size//100)\n",
    "        for i in range(0, valid_size, 100):\n",
    "            fd={model.data:valid_values[i:i+100], model.labels:valid_labels[i:i+100]}\n",
    "            val_acc[i//100], val_loss[i//100] = session.run([model.acc, model.loss], feed_dict=fd)\n",
    "        history.append((step, val_loss.mean(), val_acc.mean()))\n",
    "        print(f'Step {step:5.0f}\\t Valid. Acc. = {val_acc.mean():5.2f}')\n",
    "        \n",
    "    session.run(model.opt, feed_dict={model.data:train_values[idxs], model.labels:train_labels[idxs]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps, loss, acc = zip(*history)\n",
    "\n",
    "fig, ax_loss = plt.subplots()\n",
    "ax_acc = ax_loss.twinx()\n",
    "\n",
    "plt.sca(ax_acc)\n",
    "plt.plot(steps, acc, '-o', color='C1')\n",
    "plt.ylabel('Accuracy [%]', color='C1');\n",
    "plt.tick_params('y', colors='C1')\n",
    "plt.ylim(0,100)\n",
    "plt.yticks([0,50,100])\n",
    "\n",
    "plt.sca(ax_loss)\n",
    "plt.plot(steps, loss, '-o', color='C0')\n",
    "plt.ylabel('Log Loss', color='C0');\n",
    "plt.tick_params('y', colors='C0')\n",
    "plt.ylim(0.01, 0.3)\n",
    "\n",
    "plt.xlim(0, max(steps))\n",
    "plt.xticks([0,num_steps//4, num_steps//2, 3*num_steps//4, num_steps])\n",
    "plt.xlabel('Training Steps')\n",
    "plt.title('Validation Loss / Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the accuracy on the test set can be evaluated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = np.zeros(valid_size//100)\n",
    "for i in range(0, test_size, 100):\n",
    "    fd={model.data:test_values[i:i+100], model.labels:test_labels[i:i+100]}\n",
    "    test_accuracy[i//100] = session.run(model.acc, feed_dict=fd)\n",
    "test_accuracy = test_accuracy.mean()\n",
    "print(f'Test Accuracy = {test_accuracy:5.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 315\n",
    "show_digit(test_values[N])\n",
    "prediction = np.argmax(session.run(model.pred, feed_dict={model.data:test_values[N:N+1]}).ravel())\n",
    "print(f'prediction={prediction}\\ttarget={test_labels[N]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()"
   ]
  }
 ],
 "metadata": {
  "_change_revision": 0,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
