{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A convolutional neural network written in pure tensorflow.\n",
    "\n",
    "**Note:** this is an updated version of my [kaggle kernel](https://www.kaggle.com/flaport/tensorflow-cnn-lb-0-98929)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "39fba1ef-ec4b-38ca-fca0-0bdb31e294f4"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6efa16c1-f457-45bb-49a3-69e11c7fae18"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "from fetch_mnist import fetch_mnist\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data parameters\n",
    "num_labels = 10 # Number of different types of labels (1-10)\n",
    "width, height = 28, 28 # width / height of the image\n",
    "depth = 1 # Number of colors in the image (greyscale)\n",
    "\n",
    "# Training Parameters\n",
    "num_steps = 20000   # Number of training steps to run\n",
    "test_size = 10000 # Test data set size\n",
    "valid_size = 10000 # Validation data set size\n",
    "batch_size = 100 # Stochastic Gradient Descent batch size\n",
    "\n",
    "# CNN Parameters\n",
    "kernel_size = 5 # Convolutional Kernel size\n",
    "kernel_depth = 32 # Convolutional Kernel depth size == Number of Convolutional Kernels\n",
    "num_hidden = 1024 # Number of hidden neurons in the fully connected layer\n",
    "\n",
    "# Optimization parameters\n",
    "learning_rate = 0.001 # Learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f731214e-9fca-f54d-91be-60475207ba64"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the custom MNIST data fetcher from `fetch_mnist.py`.\n",
    "\n",
    "As a preprocessing step, the labels are converted to a one-hot encoded sparse matrix. [One hot encoding](https://en.wikipedia.org/wiki/One-hot) is just a way of replacing each integer in the label vector with a row of zeros, except at the position specified by the integer, where you put a 1.\n",
    "\n",
    "Note that the inverse transformation of the one-hot encoding can always be performed by taking the `argmax` along `axis=1`.\n",
    "\n",
    "Secondly, the image values are specified by an integer between 0 and 255. We convert these pixel values to a float between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "cc368a9d-79d3-6e70-1732-4ebdd696a409"
   },
   "outputs": [],
   "source": [
    "train_data, test_data = fetch_mnist()\n",
    "data = np.vstack([train_data, test_data])\n",
    "np.random.shuffle(data)\n",
    "\n",
    "train_data = data[:-test_size-valid_size]\n",
    "valid_data = data[-test_size-valid_size:-test_size]\n",
    "test_data  = data[-test_size:]\n",
    "\n",
    "def get_values_labels(data):\n",
    "    labels = np.int32(data[:,0])\n",
    "    labels_onehot = np.zeros((len(labels), num_labels))\n",
    "    labels_onehot[np.arange(len(labels)),labels] = 1\n",
    "    values = np.float32(data[:,1:]).reshape(-1, height, width, depth)/255\n",
    "    return values, labels, labels_onehot\n",
    "\n",
    "train_values, train_labels, train_labels_onehot = get_values_labels(train_data)\n",
    "valid_values, valid_labels, valid_labels_onehot = get_values_labels(valid_data)\n",
    "test_values, test_labels, test_labels_onehot = get_values_labels(test_data)\n",
    "\n",
    "print(f'train data shape:\\t\\t{train_values.shape}')\n",
    "print(f'train labels (one-hot) shape:\\t{train_labels_onehot.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the different digits by writing a visualization function that reshapes the 784D train and test values into a 28x28 grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_digit(digit_array):\n",
    "    plt.imshow(digit_array.reshape(height, width), cmap='Greys')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()\n",
    "    \n",
    "show_digit(train_values[31])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now build a network with two convolutional layers, followed by two fully connected layers. We initialize the input data with placeholders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_data = tf.placeholder(tf.float32, shape=(None, height, width, depth))\n",
    "tf_labels = tf.placeholder(tf.float32, shape=(None, num_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose a 4 layered network consisting of 2 convolutional layers with weights and biases (w1, b1) and (w2,b2), followed by a fully connected hidden layer (w3,b3) with `num_hidden` hidden neurons and an output layer (w4, b4) with `num_labels` output nodes (one-hot encoding).\n",
    "\n",
    "We initialize the weights and biases such that the kernel size of the second convolutional layer is twice that of the kernel size of the first convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights & biases\n",
    "w1 = tf.Variable(tf.truncated_normal([kernel_size, kernel_size, depth, kernel_depth], stddev=0.1))\n",
    "b1 = tf.Variable(tf.zeros([kernel_depth]))\n",
    "w2 = tf.Variable(tf.truncated_normal([kernel_size, kernel_size, kernel_depth, 2*kernel_depth], stddev=0.1))\n",
    "b2 = tf.Variable(tf.constant(1.0, shape=[2*kernel_depth]))\n",
    "w3 = tf.Variable(tf.truncated_normal([width // 4 * width // 4 * 2*kernel_depth, num_hidden], stddev=0.1))\n",
    "b3 = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "w4 = tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev=0.1))\n",
    "b4 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "\n",
    "# model\n",
    "def model(data):\n",
    "    ''' Model returning the output logits. '''\n",
    "    # Convolutional layer 1\n",
    "    x = tf.nn.conv2d(data, w1, [1, 1, 1, 1], padding='SAME')\n",
    "    x = tf.nn.max_pool(x, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "    x = tf.nn.relu(x + b1)\n",
    "    # Convolutional layer 2\n",
    "    x = tf.nn.conv2d(x, w2, [1, 1, 1, 1], padding='SAME')\n",
    "    x = tf.nn.max_pool(x, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "    x = tf.nn.relu(x + b2)\n",
    "    # Fully connected layer\n",
    "    x = tf.reshape(x, (-1, width // 4 * width // 4 * 2*kernel_depth))\n",
    "    x = tf.nn.relu(tf.matmul(x, w3) + b3)\n",
    "    return tf.matmul(x, w4) + b4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We use the categorical cross entropy loss for training the model.\n",
    "\n",
    "As optimizer we could use a Gradient Descent optimizer [with or without decaying learning rate] or one of the more sophisticated (and easier to optimize) optimizers like Adam or RMSProp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "tf_pred = tf.nn.softmax(model(tf_data))\n",
    "\n",
    "# loss\n",
    "tf_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=model(tf_data), labels=tf_labels))\n",
    "\n",
    "# accuracy\n",
    "tf_acc = 100*tf.reduce_mean(tf.to_float(tf.equal(tf.argmax(tf_pred, 1), tf.argmax(tf_labels, 1))))\n",
    "\n",
    "# optimizer\n",
    "#tf_opt = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "#tf_opt = tf.train.AdamOptimizer(learning_rate)\n",
    "tf_opt = tf.train.RMSPropOptimizer(learning_rate)\n",
    "\n",
    "# trainer\n",
    "tf_step = tf_opt.minimize(tf_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e33cb0c7-51ed-02ae-ebb2-dcda12832da6"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "session = tf.Session()\n",
    "session.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4dbef346-feb4-1479-62a8-042785666e66",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "ss = ShuffleSplit(n_splits=num_steps, train_size=batch_size)\n",
    "history = [(0, np.nan, 10)] # Initial Error Measures\n",
    "for step, (idx, _) in enumerate(ss.split(train_data,train_labels), start=1):\n",
    "    fd = {tf_data:train_values[idx], tf_labels:train_labels_onehot[idx]}\n",
    "    session.run(tf_step, feed_dict=fd)\n",
    "    if step%500 == 0:\n",
    "        fd = {tf_data:valid_values, tf_labels:valid_labels_onehot}\n",
    "        valid_loss, valid_accuracy = session.run([tf_loss, tf_acc], feed_dict=fd)\n",
    "        history.append((step, valid_loss, valid_accuracy))\n",
    "        print(f'Step {step}\\t Valid. Acc. = {valid_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps, loss, acc = zip(*history)\n",
    "\n",
    "fig, ax_loss = plt.subplots()\n",
    "ax_acc = ax_loss.twinx()\n",
    "\n",
    "plt.sca(ax_acc)\n",
    "plt.plot(steps, acc, '-o', color='C1')\n",
    "plt.ylabel('Accuracy [%]', color='C1');\n",
    "plt.tick_params('y', colors='C1')\n",
    "plt.ylim(98,100)\n",
    "plt.yticks([98,99,100])\n",
    "\n",
    "plt.sca(ax_loss)\n",
    "plt.plot(steps, loss, '-o', color='C0')\n",
    "plt.ylabel('Log Loss', color='C0');\n",
    "plt.tick_params('y', colors='C0')\n",
    "plt.ylim(0.01, 0.3)\n",
    "\n",
    "plt.xlim(0, max(steps))\n",
    "plt.xticks([0,num_steps//4, num_steps//2, 3*num_steps//4, num_steps])\n",
    "plt.xlabel('Training Steps')\n",
    "plt.title('Validation Loss / Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the accuracy on the test set can be evaluated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = {tf_data:test_values, tf_labels:test_labels_onehot}\n",
    "test_accuracy = session.run(tf_acc, feed_dict=fd)\n",
    "print(f'Test Accuracy = {test_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 315\n",
    "show_digit(test_values[N])\n",
    "prediction = np.argmax(session.run(tf_pred, feed_dict={tf_data:test_values[N:N+1]}).ravel())\n",
    "print(f'prediction={prediction}\\ttarget={test_labels[N]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()"
   ]
  }
 ],
 "metadata": {
  "_change_revision": 0,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
